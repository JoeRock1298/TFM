{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPROBACION DE FIRE MEDIANTE PYTORCH Y MEDIANTE NUMPY CON MATRIX MULTIPLICATIONS\n",
    "Partial code are copied heavily from https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py  \n",
    "SqueezeNet Paper:https://arxiv.org/abs/1602.07360  \n",
    "SqueezeNet 1.1 model from https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1   \n",
    "SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "\n",
    "Vamos a trabajar ya con fire1 del bloque 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#some set up\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from time import sleep, perf_counter as pc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí tenemos el medidor entre pytorch y numpy adecuado para cada fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En esta aproximacion tengo en cuenta el tiempo de definición de capas y la carga de pesos porque se supone que entre fire y fire estas operaciones tiene que hacerse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[110008. 142518. 124078. ... 116490. 114758. 133791.]\n",
      "   [126425. 111084. 115959. ... 113224. 107807. 125483.]\n",
      "   [114836. 115841. 126020. ...  99861. 146936. 108761.]\n",
      "   ...\n",
      "   [123642. 121822. 118185. ... 120389. 133534. 122394.]\n",
      "   [114969. 130810. 104550. ... 104886. 116993. 128148.]\n",
      "   [117763. 124278. 120183. ... 110460. 136737. 111134.]]\n",
      "\n",
      "  [[ 65826.  85648.  75364. ...  70297.  69925.  80211.]\n",
      "   [ 75703.  67512.  68512. ...  67280.  65508.  74967.]\n",
      "   [ 69048.  70268.  76280. ...  60429.  88552.  65946.]\n",
      "   ...\n",
      "   [ 74389.  72096.  71774. ...  73625.  81252.  74065.]\n",
      "   [ 69495.  78339.  61231. ...  61329.  71118.  77385.]\n",
      "   [ 71132.  74531.  73815. ...  66439.  82186.  66706.]]\n",
      "\n",
      "  [[ 87356. 112410.  98045. ...  92400.  92694. 104694.]\n",
      "   [100083.  90253.  90813. ...  89698.  85544. 100443.]\n",
      "   [ 90535.  91846.  99969. ...  79715. 116726.  85044.]\n",
      "   ...\n",
      "   [ 97449.  95890.  94343. ...  94935. 105753.  97322.]\n",
      "   [ 90350. 104212.  82534. ...  83256.  92509. 100009.]\n",
      "   [ 93454.  97681.  94704. ...  87533. 107649.  86768.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[390790. 587576. 602162. ... 549102. 567234. 386572.]\n",
      "   [560418. 870509. 895949. ... 860846. 847868. 610787.]\n",
      "   [562230. 875934. 859015. ... 885446. 874340. 617895.]\n",
      "   ...\n",
      "   [561378. 854074. 863770. ... 858609. 885547. 624149.]\n",
      "   [569189. 863839. 879781. ... 866843. 872873. 625377.]\n",
      "   [390737. 589618. 607534. ... 587802. 578493. 412933.]]\n",
      "\n",
      "  [[431989. 615663. 626257. ... 578594. 595957. 392719.]\n",
      "   [534932. 812748. 839522. ... 812872. 812382. 561220.]\n",
      "   [543166. 827357. 818054. ... 852129. 836249. 572400.]\n",
      "   ...\n",
      "   [540906. 806039. 816821. ... 808396. 829589. 577984.]\n",
      "   [545390. 815410. 832780. ... 812426. 822010. 574103.]\n",
      "   [340517. 518361. 536272. ... 521613. 506560. 350499.]]\n",
      "\n",
      "  [[348337. 548724. 562159. ... 515744. 534970. 361725.]\n",
      "   [529107. 811260. 824040. ... 801279. 782603. 546977.]\n",
      "   [536722. 802276. 796728. ... 821219. 813284. 562377.]\n",
      "   ...\n",
      "   [535325. 785862. 803851. ... 797871. 819051. 560085.]\n",
      "   [538608. 798778. 816442. ... 806988. 801574. 556486.]\n",
      "   [366970. 545191. 555607. ... 539512. 542106. 389586.]]]]\n",
      "[[[110008. 142518. 124078. ... 116490. 114758. 133791.]\n",
      "  [126425. 111084. 115959. ... 113224. 107807. 125483.]\n",
      "  [114836. 115841. 126020. ...  99861. 146936. 108761.]\n",
      "  ...\n",
      "  [123642. 121822. 118185. ... 120389. 133534. 122394.]\n",
      "  [114969. 130810. 104550. ... 104886. 116993. 128148.]\n",
      "  [117763. 124278. 120183. ... 110460. 136737. 111134.]]\n",
      "\n",
      " [[ 65826.  85648.  75364. ...  70297.  69925.  80211.]\n",
      "  [ 75703.  67512.  68512. ...  67280.  65508.  74967.]\n",
      "  [ 69048.  70268.  76280. ...  60429.  88552.  65946.]\n",
      "  ...\n",
      "  [ 74389.  72096.  71774. ...  73625.  81252.  74065.]\n",
      "  [ 69495.  78339.  61231. ...  61329.  71118.  77385.]\n",
      "  [ 71132.  74531.  73815. ...  66439.  82186.  66706.]]\n",
      "\n",
      " [[ 87356. 112410.  98045. ...  92400.  92694. 104694.]\n",
      "  [100083.  90253.  90813. ...  89698.  85544. 100443.]\n",
      "  [ 90535.  91846.  99969. ...  79715. 116726.  85044.]\n",
      "  ...\n",
      "  [ 97449.  95890.  94343. ...  94935. 105753.  97322.]\n",
      "  [ 90350. 104212.  82534. ...  83256.  92509. 100009.]\n",
      "  [ 93454.  97681.  94704. ...  87533. 107649.  86768.]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[390790. 587576. 602162. ... 549102. 567234. 386572.]\n",
      "  [560418. 870509. 895949. ... 860846. 847868. 610787.]\n",
      "  [562230. 875934. 859015. ... 885446. 874340. 617895.]\n",
      "  ...\n",
      "  [561378. 854074. 863770. ... 858609. 885547. 624149.]\n",
      "  [569189. 863839. 879781. ... 866843. 872873. 625377.]\n",
      "  [390737. 589618. 607534. ... 587802. 578493. 412933.]]\n",
      "\n",
      " [[431989. 615663. 626257. ... 578594. 595957. 392719.]\n",
      "  [534932. 812748. 839522. ... 812872. 812382. 561220.]\n",
      "  [543166. 827357. 818054. ... 852129. 836249. 572400.]\n",
      "  ...\n",
      "  [540906. 806039. 816821. ... 808396. 829589. 577984.]\n",
      "  [545390. 815410. 832780. ... 812426. 822010. 574103.]\n",
      "  [340517. 518361. 536272. ... 521613. 506560. 350499.]]\n",
      "\n",
      " [[348337. 548724. 562159. ... 515744. 534970. 361725.]\n",
      "  [529107. 811260. 824040. ... 801279. 782603. 546977.]\n",
      "  [536722. 802276. 796728. ... 821219. 813284. 562377.]\n",
      "  ...\n",
      "  [535325. 785862. 803851. ... 797871. 819051. 560085.]\n",
      "  [538608. 798778. 816442. ... 806988. 801574. 556486.]\n",
      "  [366970. 545191. 555607. ... 539512. 542106. 389586.]]]\n",
      "tiempo en segundos con pytorch=  0.14871505228453316\n",
      "tiempo en segundos con numpy=  0.5584366977715399\n",
      "pytorch==numpy, total: True\n"
     ]
    }
   ],
   "source": [
    "canales_iniciales=64\n",
    "canales_contraidos=16\n",
    "canales_finales=64\n",
    "acumulado_pytorch=0\n",
    "acumulado_numpy=0\n",
    "idea=True\n",
    "count=100\n",
    "tamanyo=55\n",
    "squeeze_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "for i in range(count):\n",
    "\n",
    "    imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "    weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "    bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "    weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "    bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "    weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "    bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "    tic=pc()\n",
    "    squeeze1=nn.Conv2d(canales_iniciales, canales_contraidos, kernel_size=1, bias=False)\n",
    "    squeeze1.weight = nn.Parameter(torch.from_numpy(weights1))\n",
    "    squeeze1.bias = nn.Parameter(torch.from_numpy(bias1))    \n",
    "\n",
    "    squeeze2a=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=1, bias=False)\n",
    "    squeeze2a.weight = nn.Parameter(torch.from_numpy(weights2a))\n",
    "    squeeze2a.bias = nn.Parameter(torch.from_numpy(bias2a))\n",
    "    \n",
    "    squeeze2b=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=3, bias=False, padding=1)\n",
    "    squeeze2b.weight = nn.Parameter(torch.from_numpy(weights2b))\n",
    "    squeeze2b.bias = nn.Parameter(torch.from_numpy(bias2b))    \n",
    "\n",
    "    imagen1  = torch.from_numpy(imagen).float()\n",
    "    \n",
    "    salida1=squeeze1(imagen1)\n",
    "    salida1_activation=squeeze_activation(salida1)\n",
    "    \n",
    "    salida2a=squeeze2a(salida1_activation)\n",
    "    salida2a_activation=squeeze_activation(salida2a)\n",
    "    salida2b=squeeze2b(salida1_activation)\n",
    "    salida2b_activation=squeeze_activation(salida2b)    \n",
    "    salida2_total=torch.cat([salida2a_activation,salida2b_activation], 1)\n",
    "    salida2_total_a_numpy=salida2_total.detach().numpy()\n",
    "    toc=pc()\n",
    "    acumulado_pytorch=toc-tic+acumulado_pytorch\n",
    "    \n",
    "    \n",
    "    tic2=pc()\n",
    "    n_h=1\n",
    "    n_w=1\n",
    "    pesos1=np.transpose(weights1, axes=(1, 0, 2, 3)).reshape(canales_iniciales*n_h*n_w, -1)\n",
    "    pesos2a=np.transpose(weights2a, axes=(1, 0, 2, 3)).reshape(canales_contraidos*n_h*n_w, -1)\n",
    "\n",
    "    n_h=3\n",
    "    n_w=3\n",
    "    idea2b_pre=weights2b.reshape(-1,canales_contraidos*n_h*n_w)\n",
    "    pesos2b=np.transpose(idea2b_pre)   \n",
    "    bt, c_i = imagen.shape[0:2] \n",
    "    s=1\n",
    "    n_h=1\n",
    "    n_w=1\n",
    "    k=imagen.shape[2]-n_h+1\n",
    "    col = np.zeros((bt, c_i , k,k, n_h, n_w))\n",
    "\n",
    "    for i in range(k):\n",
    "        i_max = i + s*n_h\n",
    "        for j in range(k):\n",
    "            j_max = j + s*n_w\n",
    "            col[:, :, i, j, :, :] = imagen[:, :, i:i_max:s, j:j_max:s]\n",
    "    # col = np.transpose(col, axes=(1, 4, 5, 0, 2, 3)).reshape(c_i*n_h*n_w, -1)\n",
    "    col = col.reshape(c_i*n_h*n_w, -1)\n",
    "    imagen1_numpy=np.transpose(col)\n",
    "    salida1_obtenida= np.dot(imagen1_numpy,pesos1)+bias1\n",
    "    salida1_obtenida_activacion=np.maximum(0,salida1_obtenida)\n",
    "    imagen1_obtenida = np.transpose(salida1_obtenida_activacion, axes=(1,0)).reshape(canales_contraidos,tamanyo,tamanyo)\n",
    "    imagen1_obtenida_ampliada=np.pad(imagen1_obtenida,((0,0),(1,1),(1,1)))\n",
    "    salida2a_obtenida= np.dot(salida1_obtenida,pesos2a)+bias2a\n",
    "    salida2a_obtenida_activacion=np.maximum(0,salida2a_obtenida)\n",
    "    c_i = imagen1_obtenida_ampliada.shape[0] \n",
    "    n_h=3\n",
    "    n_w=3\n",
    "    k=imagen1_obtenida_ampliada.shape[1]-n_h+1\n",
    "    col = np.zeros(( c_i , k,k, n_h, n_w))\n",
    "\n",
    "    for i in range(k):\n",
    "        i_max = i + s*n_h\n",
    "        for j in range(k):\n",
    "            j_max = j + s*n_w\n",
    "            col[ :, i, j, :, :] = imagen1_obtenida_ampliada[ :, i:i_max:s, j:j_max:s]\n",
    "    col = np.transpose(col, axes=(0, 3, 4, 1, 2)).reshape(c_i*n_h*n_w, -1)\n",
    "    #col = col.reshape(c_i*n_h*n_w, -1)\n",
    "    salida_obtenida_2b=np.transpose(col)\n",
    "    salida2b_obtenida= np.dot(salida_obtenida_2b,pesos2b)+bias2b\n",
    "    salida2b_obtenida_activacion=np.maximum(0,salida2b_obtenida)\n",
    "    salida2_total_numpy=np.concatenate((salida2a_obtenida_activacion,salida2b_obtenida_activacion), axis=1)\n",
    "    imagen_total_obtenida = np.transpose(salida2_total_numpy, axes=(1,0)).reshape(canales_finales*2,tamanyo,tamanyo)\n",
    "    toc2=pc()\n",
    "    acumulado_numpy=toc2-tic2+acumulado_numpy\n",
    "    idea&=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "print(salida2_total_a_numpy)\n",
    "print(imagen_total_obtenida)\n",
    "print (\"tiempo en segundos con pytorch= \", acumulado_pytorch/count)\n",
    "print (\"tiempo en segundos con numpy= \", acumulado_numpy/count)\n",
    "print (\"pytorch==numpy, total:\",idea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veamos ahora fire con opencl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a hacer la medición con opencl definido por equipo de squezeenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: OpenCL preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenCL setup\n",
    "import pyopencl as cl\n",
    "# import deviceinfo\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The pyopencl.ipython_ext extension is already loaded. To reload it, use:\n",
      "  %reload_ext pyopencl.ipython_ext\n"
     ]
    }
   ],
   "source": [
    "cl.get_platforms()\n",
    "cl.create_some_context()\n",
    "%load_ext pyopencl.ipython_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<pyopencl.Device 'de10_nano_sharedonly : Cyclone V SoC Development Kit' on 'Intel(R) FPGA SDK for OpenCL(TM)' at 0x-5e912690>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reprogramming device [0] with handle 26\n"
     ]
    }
   ],
   "source": [
    "context = cl.create_some_context()\n",
    "print (context.devices)\n",
    "for dev in context.devices:\n",
    "    assert dev.local_mem_size > 0\n",
    "queue = cl.CommandQueue(context)\n",
    "    \n",
    "PYOPENCL_COMPILER_OUTPUT=1\n",
    "dev =cl.get_platforms()[0].get_devices()\n",
    "binary = open(\"/root/notebooks/squeezenet/squeezenet_v10.aocx\", \"rb\").read()\n",
    "#binary = open(\"../pyopencl_projects/matrix_mult_1_single_worker_item_2.aocx\", \"rb\").read()\n",
    "        #create the program\n",
    "program= cl.Program(context, dev,[binary]).build()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanyo=55\n",
    "canales_iniciales=128\n",
    "canales_contraidos=16\n",
    "canales_finales=64\n",
    "imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "\n",
    "weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: import parameters from pytorch implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5. 7. 9. ... 3. 7. 3.]\n",
      "[7. 2. 8. 8. 3. 6. 8. 9. 7. 6. 7. 1. 5. 4. 0. 3.]\n"
     ]
    }
   ],
   "source": [
    "# params = model.state_dict() \n",
    "\n",
    "# #remove # to see the params index\n",
    "# for k,v in params.items():\n",
    "#     #print parameter name\n",
    "#     print(k,params[k].numpy().shape)\n",
    "\n",
    "# conv1_weight = params['features.0.weight'].numpy().reshape(-1)\n",
    "# conv1_bias = params['features.0.bias'].numpy()\n",
    "\n",
    "#fire - fire - maxpool block 1\n",
    "fire1_squeeze_weight = weights1.reshape(-1)\n",
    "print(fire1_squeeze_weight)\n",
    "fire1_squeeze_bias = bias1\n",
    "print(fire1_squeeze_bias)\n",
    "fire1_expand1x1_weight = weights2a.reshape(-1)\n",
    "fire1_expand1x1_bias = bias2a\n",
    "\n",
    "fire1_expand3x3_weight =weights2b.reshape(-1)\n",
    "fire1_expand3x3_bias = bias2b\n",
    "\n",
    "# fire2_squeeze_weight = params['features.4.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire2_squeeze_bias = params['features.4.squeeze.bias'].numpy()\n",
    "\n",
    "# fire2_expand1x1_weight = params['features.4.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire2_expand1x1_bias = params['features.4.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire2_expand3x3_weight = params['features.4.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire2_expand3x3_bias = params['features.4.expand3x3.bias'].numpy()\n",
    "\n",
    "# #fire - fire - maxpool block 2\n",
    "# fire3_squeeze_weight = params['features.6.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire3_squeeze_bias = params['features.6.squeeze.bias'].numpy()\n",
    "\n",
    "# fire3_expand1x1_weight = params['features.6.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire3_expand1x1_bias = params['features.6.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire3_expand3x3_weight = params['features.6.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire3_expand3x3_bias = params['features.6.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire4_squeeze_weight = params['features.7.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire4_squeeze_bias = params['features.7.squeeze.bias'].numpy()\n",
    "\n",
    "# fire4_expand1x1_weight = params['features.7.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire4_expand1x1_bias = params['features.7.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire4_expand3x3_weight = params['features.7.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire4_expand3x3_bias = params['features.7.expand3x3.bias'].numpy()\n",
    "\n",
    "# #fire - fire - fire - fire block 3\n",
    "# fire5_squeeze_weight = params['features.9.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire5_squeeze_bias = params['features.9.squeeze.bias'].numpy()\n",
    "\n",
    "# fire5_expand1x1_weight = params['features.9.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire5_expand1x1_bias = params['features.9.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire5_expand3x3_weight = params['features.9.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire5_expand3x3_bias = params['features.9.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire6_squeeze_weight = params['features.10.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire6_squeeze_bias = params['features.10.squeeze.bias'].numpy()\n",
    "\n",
    "# fire6_expand1x1_weight = params['features.10.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire6_expand1x1_bias = params['features.10.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire6_expand3x3_weight = params['features.10.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire6_expand3x3_bias = params['features.10.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire7_squeeze_weight = params['features.11.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire7_squeeze_bias = params['features.11.squeeze.bias'].numpy()\n",
    "\n",
    "# fire7_expand1x1_weight = params['features.11.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire7_expand1x1_bias = params['features.11.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire7_expand3x3_weight = params['features.11.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire7_expand3x3_bias = params['features.11.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire8_squeeze_weight = params['features.12.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire8_squeeze_bias = params['features.12.squeeze.bias'].numpy()\n",
    "\n",
    "# fire8_expand1x1_weight = params['features.12.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire8_expand1x1_bias = params['features.12.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire8_expand3x3_weight = params['features.12.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire8_expand3x3_bias = params['features.12.expand3x3.bias'].numpy()\n",
    "\n",
    "# classifier_conv_weight = params['classifier.1.weight'].numpy().reshape(-1)\n",
    "# classifier_conv_bias = params['classifier.1.bias'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat OpenCL memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 6 9 ... 3 7 6]\n",
      "(387200,)\n"
     ]
    }
   ],
   "source": [
    "h_sample = imagen.reshape(-1)\n",
    "print(h_sample)\n",
    "print(h_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "h_sample = imagen.reshape(-1).astype(np.float32)\n",
    "# h_result_conv = np.empty(1 * 64 * 111 * 111).astype(np.float32)\n",
    "# h_result_pool1 = np.empty(1 * 64 * 55 * 55).astype(np.float32)\n",
    "\n",
    "h_result_fire1_squeeze = np.empty(1 * canales_contraidos * tamanyo * tamanyo).astype(np.float32)\n",
    "h_result_fire1_expand = np.empty(1 * 2*canales_finales * tamanyo* tamanyo).astype(np.float32)\n",
    "\n",
    "# h_result_fire2_squeeze = np.empty(1 * 16 * 55 * 55).astype(np.float32)\n",
    "# h_result_fire2_expand = np.empty(1 * 128 * 55 * 55).astype(np.float32)\n",
    "# h_result_pool2 = np.empty(1 * 128 * 27 * 27).astype(np.float32)\n",
    "\n",
    "# h_result_fire3_squeeze = np.empty(1 * 32 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire3_expand = np.empty(1 * 256 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire4_squeeze = np.empty(1 * 32 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire4_expand = np.empty(1 * 256 * 27 * 27).astype(np.float32)\n",
    "# h_result_pool3 = np.empty(1 * 256 * 13 * 13).astype(np.float32)\n",
    "\n",
    "# h_result_fire5_squeeze = np.empty(1 * 48 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire5_expand = np.empty(1 * 384 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire6_squeeze = np.empty(1 * 48 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire6_expand = np.empty(1 * 384 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire7_squeeze = np.empty(1 * 64 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire7_expand = np.empty(1 * 512 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire8_squeeze = np.empty(1 * 64 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire8_expand = np.empty(1 * 512 * 13 * 13).astype(np.float32)\n",
    "\n",
    "# h_result_classifier_conv = np.empty(1 * 1000 * 13 * 13).astype(np.float32)\n",
    "# h_result_classifier = np.empty(1 * 1000).astype(np.float32)\n",
    "\n",
    "# device input buffer\n",
    "d_sample = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=h_sample)\n",
    "\n",
    "# device conv1 buffers \n",
    "# d_conv1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_weight)\n",
    "# d_conv1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_bias)\n",
    "\n",
    "# d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights1)\n",
    "# d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias1)\n",
    "# d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2a)\n",
    "# d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2a)\n",
    "# d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2b)\n",
    "# d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2b)\n",
    "\n",
    "#print(d_fire1_squeeze_weight)\n",
    "d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_weight)\n",
    "d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_bias)\n",
    "d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_weight)\n",
    "d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_bias)\n",
    "d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_weight)\n",
    "d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_bias)\n",
    "\n",
    "# d_fire2_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_squeeze_weight)\n",
    "# d_fire2_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_squeeze_bias)\n",
    "# d_fire2_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand1x1_weight)\n",
    "# d_fire2_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand1x1_bias)\n",
    "# d_fire2_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand3x3_weight)\n",
    "# d_fire2_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand3x3_bias)\n",
    "\n",
    "# d_fire3_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_squeeze_weight)\n",
    "# d_fire3_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_squeeze_bias)\n",
    "# d_fire3_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand1x1_weight)\n",
    "# d_fire3_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand1x1_bias)\n",
    "# d_fire3_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand3x3_weight)\n",
    "# d_fire3_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand3x3_bias)\n",
    "\n",
    "# d_fire4_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_squeeze_weight)\n",
    "# d_fire4_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_squeeze_bias)\n",
    "# d_fire4_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand1x1_weight)\n",
    "# d_fire4_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand1x1_bias)\n",
    "# d_fire4_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand3x3_weight)\n",
    "# d_fire4_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand3x3_bias)\n",
    "\n",
    "# d_fire5_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_squeeze_weight)\n",
    "# d_fire5_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_squeeze_bias)\n",
    "# d_fire5_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand1x1_weight)\n",
    "# d_fire5_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand1x1_bias)\n",
    "# d_fire5_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand3x3_weight)\n",
    "# d_fire5_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand3x3_bias)\n",
    "\n",
    "# d_fire6_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_squeeze_weight)\n",
    "# d_fire6_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_squeeze_bias)\n",
    "# d_fire6_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand1x1_weight)\n",
    "# d_fire6_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand1x1_bias)\n",
    "# d_fire6_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand3x3_weight)\n",
    "# d_fire6_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand3x3_bias)\n",
    "\n",
    "# d_fire7_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_squeeze_weight)\n",
    "# d_fire7_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_squeeze_bias)\n",
    "# d_fire7_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand1x1_weight)\n",
    "# d_fire7_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand1x1_bias)\n",
    "# d_fire7_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand3x3_weight)\n",
    "# d_fire7_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand3x3_bias)\n",
    "\n",
    "# d_fire8_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_squeeze_weight)\n",
    "# d_fire8_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_squeeze_bias)\n",
    "# d_fire8_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand1x1_weight)\n",
    "# d_fire8_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand1x1_bias)\n",
    "# d_fire8_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand3x3_weight)\n",
    "# d_fire8_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand3x3_bias)\n",
    "\n",
    "# d_classifier_conv_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=classifier_conv_weight)\n",
    "# d_classifier_conv_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=classifier_conv_bias)\n",
    "\n",
    "# d_result_conv = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_conv.nbytes)\n",
    "# d_result_pool1 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool1.nbytes)\n",
    "\n",
    "d_result_fire1_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire1_squeeze.nbytes)\n",
    "d_result_fire1_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire1_expand.nbytes)\n",
    "# d_result_fire2_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire2_squeeze.nbytes)\n",
    "# d_result_fire2_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire2_expand.nbytes)\n",
    "# d_result_pool2 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool2.nbytes)\n",
    "\n",
    "# d_result_fire3_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire3_squeeze.nbytes)\n",
    "# d_result_fire3_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire3_expand.nbytes)\n",
    "# d_result_fire4_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire4_squeeze.nbytes)\n",
    "# d_result_fire4_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire4_expand.nbytes)\n",
    "# d_result_pool3 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool3.nbytes)\n",
    "\n",
    "# d_result_fire5_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire5_squeeze.nbytes)\n",
    "# d_result_fire5_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire5_expand.nbytes)\n",
    "# d_result_fire6_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire6_squeeze.nbytes)\n",
    "# d_result_fire6_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire6_expand.nbytes)\n",
    "# d_result_fire7_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire7_squeeze.nbytes)\n",
    "# d_result_fire7_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire7_expand.nbytes)\n",
    "# d_result_fire8_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire8_squeeze.nbytes)\n",
    "# d_result_fire8_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire8_expand.nbytes)\n",
    "\n",
    "# d_result_classifier_conv = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_classifier_conv.nbytes)\n",
    "# d_result_classifier = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_classifier.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: creat kernels\n",
    "Creat & build program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kernelSource = open(\"squeezenet.cl\").read()\n",
    "#program = cl.Program(context, kernelSource).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv3x3 = program.conv2d3x3\n",
    "conv3x3.set_scalar_arg_dtypes([np.int32, np.int32, np.int32, np.int32, np.int32, np.int32, \\\n",
    "                               None, None, None, None])\n",
    "\n",
    "maxpool = program.maxpool2d\n",
    "maxpool.set_scalar_arg_dtypes([np.int32, np.int32, None, None])\n",
    "\n",
    "conv1x1 = program.conv2d1x1\n",
    "conv1x1.set_scalar_arg_dtypes([np.int32, np.int32, \\\n",
    "                               None, None, None, None])\n",
    "\n",
    "avgpool = program.avgpool2d\n",
    "avgpool.set_scalar_arg_dtypes([None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCL kernel: squeezenet.cl\n",
    "conv2d3x3: 2-D 3x3 convolution.  \n",
    "conv2d1x1: 2-D 1x1 convolution. kerner size 1, stride 1  \n",
    "maxpool2d: 2-D max pool. kerner size 3, stride 2  \n",
    "avgpool2d: 2-D average pool. kerner size 13\n",
    "```C\n",
    "//maxPool2d \n",
    "//kernel_size=3 stride=2\n",
    "//output one feature map per kernel\n",
    "__kernel void maxpool2d(\n",
    "\tconst int input_size,\n",
    "\tconst int output_size,\n",
    "\t__global float *input_im,\n",
    "    __global float *restrict output_im)\n",
    "{\n",
    "\tint channel = get_global_id(0);//get output channel index\n",
    "\t\n",
    "\tinput_im += channel * input_size * input_size;\n",
    "\toutput_im += channel * output_size * output_size;\n",
    "\n",
    "\t//loop over output feature map\n",
    "\tfor(int i = 0; i < output_size; i++)//row\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < output_size; j++)//col\n",
    "\t\t{\n",
    "\t\t\t//find the max value in 3x3 reigon \n",
    "\t\t\t//to be one element in the output feature map\n",
    "\t\t\tfloat tmp = 0.0;\n",
    "\n",
    "\t\t\tfor(int k = 0; k < 3; k++)//row\n",
    "\t\t\t{\n",
    "\t\t\t\tfor(int l = 0; l < 3; l++)//col\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tfloat value = input_im[(i * 2 + k) * input_size  + j * 2 + l ];\n",
    "\t\t\t\t\tif(value > tmp)\n",
    "\t\t\t\t\t\ttmp = value;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\t//store the result to output feature map\n",
    "\t\t\toutput_im[i * output_size + j] = tmp; \n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//3x3 convolution layer\n",
    "//output one feature map per kernel\n",
    "__kernel void conv2d3x3(\n",
    "\tconst int input_channels, const int input_size,\n",
    "\tconst int pad, const int stride,\n",
    "\tconst int start_channel, //start_channel is for 1x1 feature map in fire layer\n",
    "\tconst int output_size,\n",
    "\t__global float *restrict input_im,\n",
    "\t__global const float *restrict  filter_weight,\n",
    "\t__global const float *restrict  filter_bias,\n",
    "\t__global float *restrict output_im\n",
    "\t)\n",
    "{\n",
    "\tint filter_index = get_global_id(0); //get output channel index\n",
    "\n",
    "\tfilter_weight += filter_index * input_channels * 9;\n",
    "\tfloat bias = filter_bias[filter_index];\n",
    "\toutput_im += (start_channel + filter_index) * output_size * output_size;\n",
    "\t\n",
    "\t//loop over output feature map\n",
    "\tfor(int i = 0; i < output_size; i++)\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < output_size; j++)\n",
    "\t\t{\n",
    "\t\t\t//compute one element in the output feature map\n",
    "\t\t\tfloat tmp = bias;\n",
    "\t\t\t\n",
    "\t\t\t//compute dot product of 2 input_channels x 3 x 3 matrix\n",
    "\t\t\tfor(int k = 0; k < input_channels; k++)\n",
    "\t\t\t{\n",
    "\t\t\t\tfor(int l = 0; l < 3; l++)\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tint h = i * stride + l - pad;\n",
    "\t\t\t\t\tfor(int m = 0; m < 3; m++)\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\tint w = j * stride + m - pad;\n",
    "\t\t\t\t\t\tif((h >= 0) && (h < input_size) && (w >= 0) && (w < input_size))\n",
    "\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\ttmp += input_im[k * input_size * input_size + (i * stride + l - pad) * input_size + j \n",
    "                                   * stride + m - pad] * filter_weight[9 * k + 3 * l + m];\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t//add relu activation after conv\n",
    "\t\t\toutput_im[i * output_size + j] = (tmp > 0.0) ? tmp : 0.0;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//1x1 convolution layer\n",
    "//output one feature map per kernel\n",
    "__kernel void conv2d1x1(\n",
    "\tconst int input_channels, const int input_size,\n",
    "\t__global float *input_im,\n",
    "\t__global const float *restrict filter_weight,\n",
    "\t__global const float *restrict filter_bias,\n",
    "\t__global float *restrict output_im)\n",
    "{\n",
    "\tint filter_index = get_global_id(0); // 0 - (output_channels - 1)\n",
    "\n",
    "\tfilter_weight += filter_index * input_channels;\n",
    "\n",
    "\tfloat bias = filter_bias[filter_index];\n",
    "\t\n",
    "\toutput_im += filter_index * input_size * input_size;//start_channel is for 1x1 feature map in fire layer\n",
    "\n",
    "\t//loop over output feature map\n",
    "\t//out\n",
    "\tfor(int i = 0; i < input_size; i++)\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < input_size; j++)\n",
    "\t\t{\n",
    "\t\t\tfloat tmp = bias;\n",
    "\t\t\tfor(int k = 0; k < input_channels; k++)\n",
    "\t\t\t{\n",
    "\t\t\t\ttmp += input_im[k * input_size * input_size + i * input_size + j] * filter_weight[k];\n",
    "\t\t\t}\n",
    "\t\t\t//add relu after conv\n",
    "\t\t\toutput_im[i * input_size + j] = (tmp > 0.0) ? tmp : 0.0;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//last layer use a 13 x 13 avgPool layer as classifier\n",
    "//one class score per kernel\n",
    "__kernel void avgpool2d(\n",
    "\t__global float *restrict input_im,\n",
    "\t__global float *restrict output_im)\n",
    "{\n",
    "\tint class_index = get_global_id(0);//get class score index\n",
    "\n",
    "\tinput_im += 169 * class_index;\n",
    "\t\n",
    "\tfloat tmp = 0.0f;\n",
    "\n",
    "\tfor(int i = 0; i < 169; i++)\n",
    "\t{\n",
    "\t\ttmp += input_im[i];\n",
    "\t}\n",
    "\n",
    "\toutput_im[class_index] = tmp / 169.0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run OpenCL implement  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyOpenCL WARNING: a clean-up operation failed (dead context maybe?)\n",
      "clReleaseEvent failed with code -58\n",
      "PyOpenCL WARNING: a clean-up operation failed (dead context maybe?)\n",
      "clReleaseEvent failed with code -58\n",
      "PyOpenCL WARNING: a clean-up operation failed (dead context maybe?)\n",
      "clReleaseEvent failed with code -58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo en segundos con pytorch=  0.1605777390068397\n",
      "tiempo en segundos con numpy=  0.5780589519999921\n",
      "tiempo en segundos con opencl= 0.360246701980941\n",
      "comparativa (pytorch==numpy): True\n",
      "comparativa (pytorch==opencl): True\n"
     ]
    }
   ],
   "source": [
    "imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "\n",
    "weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)  \n",
    "\n",
    "tic=pc()\n",
    "squeeze1=nn.Conv2d(canales_iniciales, canales_contraidos, kernel_size=1, bias=False)\n",
    "squeeze1.weight = nn.Parameter(torch.from_numpy(weights1))\n",
    "squeeze1.bias = nn.Parameter(torch.from_numpy(bias1))\n",
    "\n",
    "squeeze2a=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=1, bias=False)\n",
    "squeeze2a.weight = nn.Parameter(torch.from_numpy(weights2a))\n",
    "squeeze2a.bias = nn.Parameter(torch.from_numpy(bias2a))\n",
    "\n",
    "squeeze2b=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=3, bias=False, padding=1)\n",
    "squeeze2b.weight = nn.Parameter(torch.from_numpy(weights2b))\n",
    "squeeze2b.bias = nn.Parameter(torch.from_numpy(bias2b))\n",
    "\n",
    "\n",
    "imagen1  = torch.from_numpy(imagen).float()\n",
    "\n",
    "salida1=squeeze1(imagen1)\n",
    "salida1_activation=squeeze_activation(salida1)\n",
    "\n",
    "salida2a=squeeze2a(salida1_activation)\n",
    "salida2a_activation=squeeze_activation(salida2a)\n",
    "salida2b=squeeze2b(salida1_activation)\n",
    "salida2b_activation=squeeze_activation(salida2b)    \n",
    "salida2_total=torch.cat([salida2a_activation,salida2b_activation], 1)\n",
    "salida2_total_a_numpy=salida2_total.detach().numpy()\n",
    "toc=pc()\n",
    "acumulado_pytorch=toc-tic+acumulado_pytorch\n",
    "\n",
    "\n",
    "tic2=pc()\n",
    "n_h=1\n",
    "n_w=1\n",
    "pesos1=np.transpose(weights1, axes=(1, 0, 2, 3)).reshape(canales_iniciales*n_h*n_w, -1)\n",
    "pesos2a=np.transpose(weights2a, axes=(1, 0, 2, 3)).reshape(canales_contraidos*n_h*n_w, -1)\n",
    "\n",
    "n_h=3\n",
    "n_w=3\n",
    "idea2b_pre=weights2b.reshape(-1,canales_contraidos*n_h*n_w)\n",
    "pesos2b=np.transpose(idea2b_pre)   \n",
    "bt, c_i = imagen.shape[0:2] \n",
    "s=1\n",
    "n_h=1\n",
    "n_w=1\n",
    "k=imagen.shape[2]-n_h+1\n",
    "col = np.zeros((bt, c_i , k,k, n_h, n_w))\n",
    "\n",
    "for i in range(k):\n",
    "    i_max = i + s*n_h\n",
    "    for j in range(k):\n",
    "        j_max = j + s*n_w\n",
    "        col[:, :, i, j, :, :] = imagen[:, :, i:i_max:s, j:j_max:s]\n",
    "# col = np.transpose(col, axes=(1, 4, 5, 0, 2, 3)).reshape(c_i*n_h*n_w, -1)\n",
    "col = col.reshape(c_i*n_h*n_w, -1)\n",
    "imagen1_numpy=np.transpose(col)\n",
    "salida1_obtenida= np.dot(imagen1_numpy,pesos1)+bias1\n",
    "salida1_obtenida_activacion=np.maximum(0,salida1_obtenida)\n",
    "imagen1_obtenida = np.transpose(salida1_obtenida_activacion, axes=(1,0)).reshape(canales_contraidos,tamanyo,tamanyo)\n",
    "imagen1_obtenida_ampliada=np.pad(imagen1_obtenida,((0,0),(1,1),(1,1)))\n",
    "salida2a_obtenida= np.dot(salida1_obtenida,pesos2a)+bias2a\n",
    "salida2a_obtenida_activacion=np.maximum(0,salida2a_obtenida)\n",
    "c_i = imagen1_obtenida_ampliada.shape[0] \n",
    "n_h=3\n",
    "n_w=3\n",
    "k=imagen1_obtenida_ampliada.shape[1]-n_h+1\n",
    "col = np.zeros(( c_i , k,k, n_h, n_w))\n",
    "\n",
    "for i in range(k):\n",
    "    i_max = i + s*n_h\n",
    "    for j in range(k):\n",
    "        j_max = j + s*n_w\n",
    "        col[ :, i, j, :, :] = imagen1_obtenida_ampliada[ :, i:i_max:s, j:j_max:s]\n",
    "col = np.transpose(col, axes=(0, 3, 4, 1, 2)).reshape(c_i*n_h*n_w, -1)\n",
    "#col = col.reshape(c_i*n_h*n_w, -1)\n",
    "salida_obtenida_2b=np.transpose(col)\n",
    "salida2b_obtenida= np.dot(salida_obtenida_2b,pesos2b)+bias2b\n",
    "salida2b_obtenida_activacion=np.maximum(0,salida2b_obtenida)\n",
    "salida2_total_numpy=np.concatenate((salida2a_obtenida_activacion,salida2b_obtenida_activacion), axis=1)\n",
    "imagen_total_obtenida = np.transpose(salida2_total_numpy, axes=(1,0)).reshape(canales_finales*2,tamanyo,tamanyo)\n",
    "toc2=pc()\n",
    "acumulado_numpy=toc2-tic2+acumulado_numpy\n",
    "idea&=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "\n",
    "\n",
    "# device conv1 buffers \n",
    "# d_conv1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_weight)\n",
    "# d_conv1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_bias)\n",
    "\n",
    "# d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights1)\n",
    "# d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias1)\n",
    "# d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2a)\n",
    "# d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2a)\n",
    "# d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2b)\n",
    "# d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2b)\n",
    "\n",
    "h_sample = imagen.reshape(-1).astype(np.float32)\n",
    "d_sample = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=h_sample)\n",
    "\n",
    "fire1_squeeze_weight = weights1.reshape(-1)\n",
    "fire1_squeeze_bias = bias1\n",
    "fire1_expand1x1_weight = weights2a.reshape(-1)\n",
    "fire1_expand1x1_bias = bias2a\n",
    "fire1_expand3x3_weight =weights2b.reshape(-1)\n",
    "fire1_expand3x3_bias = bias2b\n",
    "\n",
    "d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_weight)\n",
    "d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_bias)\n",
    "d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_weight)\n",
    "d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_bias)\n",
    "d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_weight)\n",
    "d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_bias)\n",
    "\n",
    "tic3 = pc()\n",
    "#first conv layer\n",
    "# conv3x3(queue,(64,), None, 3, 224, 0, 2, 0, 111, d_sample, d_conv1_weight, d_conv1_bias, d_result_conv)\n",
    "# maxpool(queue,(64,), None, 111, 55, d_result_conv, d_result_pool1)\n",
    "\n",
    "#block1\n",
    "event1=conv1x1(queue,(canales_contraidos,), None, canales_iniciales, tamanyo, d_sample, d_fire1_squeeze_weight, d_fire1_squeeze_bias, d_result_fire1_squeeze)\n",
    "event2=conv1x1(queue,(canales_finales,), None, canales_contraidos, tamanyo, d_result_fire1_squeeze, d_fire1_expand1x1_weight, d_fire1_expand1x1_bias, d_result_fire1_expand)\n",
    "event3=conv3x3(queue,(canales_finales,), None, canales_contraidos, tamanyo, 1, 1, canales_finales, tamanyo, d_result_fire1_squeeze, d_fire1_expand3x3_weight, d_fire1_expand3x3_bias, d_result_fire1_expand)\n",
    "\n",
    "# conv1x1(queue,(16,), None, 128, 55, d_result_fire1_expand, d_fire2_squeeze_weight, d_fire2_squeeze_bias, d_result_fire2_squeeze)\n",
    "# conv1x1(queue,(64,), None, 16, 55, d_result_fire2_squeeze, d_fire2_expand1x1_weight, d_fire2_expand1x1_bias, d_result_fire2_expand)\n",
    "# conv3x3(queue,(64,), None, 16, 55, 1, 1, 64, 55, d_result_fire2_squeeze, d_fire2_expand3x3_weight, d_fire2_expand3x3_bias, d_result_fire2_expand)\n",
    "\n",
    "# maxpool(queue,(128,), None, 55, 27, d_result_fire2_expand, d_result_pool2)\n",
    "\n",
    "# #block2\n",
    "# conv1x1(queue,(32,), None, 128, 27, d_result_pool2, d_fire3_squeeze_weight, d_fire3_squeeze_bias, d_result_fire3_squeeze)\n",
    "# conv1x1(queue,(128,), None, 32, 27, d_result_fire3_squeeze, d_fire3_expand1x1_weight, d_fire3_expand1x1_bias, d_result_fire3_expand)\n",
    "# conv3x3(queue,(128,), None, 32, 27, 1, 1, 128, 27, d_result_fire3_squeeze, d_fire3_expand3x3_weight, d_fire3_expand3x3_bias, d_result_fire3_expand)\n",
    "\n",
    "# conv1x1(queue,(32,), None, 256, 27, d_result_fire3_expand, d_fire4_squeeze_weight, d_fire4_squeeze_bias, d_result_fire4_squeeze)\n",
    "# conv1x1(queue,(128,), None, 32, 27, d_result_fire4_squeeze, d_fire4_expand1x1_weight, d_fire4_expand1x1_bias, d_result_fire4_expand)\n",
    "# conv3x3(queue,(128,), None, 32, 27, 1, 1, 128, 27, d_result_fire4_squeeze, d_fire4_expand3x3_weight, d_fire4_expand3x3_bias, d_result_fire4_expand)\n",
    "\n",
    "# maxpool(queue,(256,), None, 27, 13, d_result_fire4_expand, d_result_pool3)\n",
    "\n",
    "# #block3\n",
    "# conv1x1(queue,(48,), None, 256, 13, d_result_pool3, d_fire5_squeeze_weight, d_fire5_squeeze_bias, d_result_fire5_squeeze)\n",
    "# conv1x1(queue,(192,), None, 48, 13, d_result_fire5_squeeze, d_fire5_expand1x1_weight, d_fire5_expand1x1_bias, d_result_fire5_expand)\n",
    "# conv3x3(queue,(192,), None, 48, 13, 1, 1, 192, 13, d_result_fire5_squeeze, d_fire5_expand3x3_weight, d_fire5_expand3x3_bias, d_result_fire5_expand)\n",
    "\n",
    "# conv1x1(queue,(48,), None, 384, 13, d_result_fire5_expand, d_fire6_squeeze_weight, d_fire6_squeeze_bias, d_result_fire6_squeeze)\n",
    "# conv1x1(queue,(192,), None, 48, 13, d_result_fire6_squeeze, d_fire6_expand1x1_weight, d_fire6_expand1x1_bias, d_result_fire6_expand)\n",
    "# conv3x3(queue,(192,), None, 48, 13, 1, 1, 192, 13, d_result_fire6_squeeze, d_fire6_expand3x3_weight, d_fire6_expand3x3_bias, d_result_fire6_expand)\n",
    "\n",
    "# conv1x1(queue,(64,), None, 384, 13, d_result_fire6_expand, d_fire7_squeeze_weight, d_fire7_squeeze_bias, d_result_fire7_squeeze)\n",
    "# conv1x1(queue,(256,), None, 64, 13, d_result_fire7_squeeze, d_fire7_expand1x1_weight, d_fire7_expand1x1_bias, d_result_fire7_expand)\n",
    "# conv3x3(queue,(256,), None, 64, 13, 1, 1, 256, 13, d_result_fire7_squeeze, d_fire7_expand3x3_weight, d_fire7_expand3x3_bias, d_result_fire7_expand)\n",
    "\n",
    "# conv1x1(queue,(64,), None, 512, 13, d_result_fire7_expand, d_fire8_squeeze_weight, d_fire8_squeeze_bias, d_result_fire8_squeeze)\n",
    "# conv1x1(queue,(256,), None, 64, 13, d_result_fire8_squeeze, d_fire8_expand1x1_weight, d_fire8_expand1x1_bias, d_result_fire8_expand)\n",
    "# conv3x3(queue,(256,), None, 64, 13, 1, 1, 256, 13, d_result_fire8_squeeze, d_fire8_expand3x3_weight, d_fire8_expand3x3_bias, d_result_fire8_expand)\n",
    "\n",
    "# # classifier\n",
    "# conv1x1(queue,(1000,), None, 512, 13, d_result_fire8_expand, d_classifier_conv_weight, d_classifier_conv_bias, d_result_classifier_conv)\n",
    "# avgpool(queue,(1000,), None, d_result_classifier_conv, d_result_classifier)\n",
    "# # Wait for the commands to finish before reading back\n",
    "# event1.wait()\n",
    "# cl.enqueue_copy(queue, h_result_fire1_squeeze,  d_result_fire1_squeeze)\n",
    "event3.wait()\n",
    "#queue.finish()\n",
    "\n",
    "cl.enqueue_copy(queue, h_result_fire1_expand, d_result_fire1_expand)\n",
    "# queue.finish()\n",
    "veamos=h_result_fire1_expand.reshape(-1,tamanyo,tamanyo)\n",
    "# veamos2=h_result_fire1_squeeze.reshape(-1,tamanyo,tamanyo)\n",
    "rtime = pc() - tic3\n",
    "print (\"tiempo en segundos con pytorch= \", toc-tic)\n",
    "print (\"tiempo en segundos con numpy= \", toc2-tic2)\n",
    "print (\"tiempo en segundos con opencl=\",rtime)\n",
    "\n",
    "comparativa1=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "comparativa2=np.allclose(veamos, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "print(\"comparativa (pytorch==numpy):\",comparativa1)\n",
    "print(\"comparativa (pytorch==opencl):\",comparativa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[176587. 173423. 186209. ... 192468. 187182. 184263.]\n",
      " [177170. 195264. 180268. ... 175381. 187080. 169815.]\n",
      " [177985. 176837. 167691. ... 196703. 187880. 163740.]\n",
      " ...\n",
      " [189196. 173456. 178776. ... 186218. 196836. 190622.]\n",
      " [175428. 190246. 187971. ... 181575. 187692. 191665.]\n",
      " [199281. 184936. 164862. ... 175103. 182874. 195998.]]\n",
      "[[176587. 173423. 186209. ... 192468. 187182. 184263.]\n",
      " [177170. 195264. 180268. ... 175381. 187080. 169815.]\n",
      " [177985. 176837. 167691. ... 196703. 187880. 163740.]\n",
      " ...\n",
      " [189196. 173456. 178776. ... 186218. 196836. 190622.]\n",
      " [175428. 190246. 187971. ... 181575. 187692. 191665.]\n",
      " [199281. 184936. 164862. ... 175103. 182874. 195998.]]\n",
      "[[[[5 1 1 ... 8 3 1]\n",
      "   [6 9 9 ... 8 2 1]\n",
      "   [1 3 3 ... 9 4 6]\n",
      "   ...\n",
      "   [7 8 4 ... 2 5 3]\n",
      "   [0 4 3 ... 0 0 6]\n",
      "   [6 2 5 ... 7 6 8]]\n",
      "\n",
      "  [[4 1 8 ... 4 3 1]\n",
      "   [2 4 1 ... 0 2 2]\n",
      "   [6 4 9 ... 3 1 8]\n",
      "   ...\n",
      "   [3 9 1 ... 8 0 0]\n",
      "   [9 7 1 ... 1 0 8]\n",
      "   [2 2 0 ... 6 2 1]]\n",
      "\n",
      "  [[3 4 3 ... 1 4 2]\n",
      "   [2 4 7 ... 1 6 6]\n",
      "   [2 6 1 ... 5 2 3]\n",
      "   ...\n",
      "   [7 4 1 ... 2 5 6]\n",
      "   [6 4 8 ... 0 9 2]\n",
      "   [7 9 6 ... 7 6 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[8 3 9 ... 3 9 1]\n",
      "   [4 9 9 ... 6 9 0]\n",
      "   [0 2 4 ... 8 8 3]\n",
      "   ...\n",
      "   [5 4 5 ... 1 9 8]\n",
      "   [5 8 9 ... 4 4 8]\n",
      "   [8 0 1 ... 7 7 6]]\n",
      "\n",
      "  [[1 7 5 ... 7 2 0]\n",
      "   [9 9 3 ... 8 9 2]\n",
      "   [5 3 1 ... 7 8 3]\n",
      "   ...\n",
      "   [8 3 9 ... 0 5 9]\n",
      "   [0 7 2 ... 5 0 7]\n",
      "   [8 3 7 ... 6 1 2]]\n",
      "\n",
      "  [[0 7 2 ... 0 0 6]\n",
      "   [9 7 8 ... 2 6 9]\n",
      "   [6 0 3 ... 1 2 5]\n",
      "   ...\n",
      "   [6 5 1 ... 6 2 0]\n",
      "   [7 4 2 ... 0 5 2]\n",
      "   [8 1 9 ... 7 8 2]]]]\n",
      "[[[[3.]]\n",
      "\n",
      "  [[0.]]\n",
      "\n",
      "  [[1.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[7.]]\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  [[9.]]]\n",
      "\n",
      "\n",
      " [[[0.]]\n",
      "\n",
      "  [[4.]]\n",
      "\n",
      "  [[0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[3.]]\n",
      "\n",
      "  [[7.]]\n",
      "\n",
      "  [[7.]]]\n",
      "\n",
      "\n",
      " [[[0.]]\n",
      "\n",
      "  [[1.]]\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[8.]]\n",
      "\n",
      "  [[1.]]\n",
      "\n",
      "  [[3.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[1.]]\n",
      "\n",
      "  [[8.]]\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[5.]]\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  [[0.]]]\n",
      "\n",
      "\n",
      " [[[5.]]\n",
      "\n",
      "  [[8.]]\n",
      "\n",
      "  [[2.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  [[9.]]\n",
      "\n",
      "  [[3.]]]\n",
      "\n",
      "\n",
      " [[[8.]]\n",
      "\n",
      "  [[9.]]\n",
      "\n",
      "  [[3.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[6.]]\n",
      "\n",
      "  [[1.]]\n",
      "\n",
      "  [[1.]]]]\n",
      "[3. 0. 1. ... 6. 1. 1.]\n",
      "[1. 3. 7. 7. 8. 5. 8. 8. 2. 9. 4. 2. 5. 8. 9. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(imagen_total_obtenida[0])\n",
    "print(veamos[0])\n",
    "print(imagen)\n",
    "print(weights1)\n",
    "print(fire1_squeeze_weight)\n",
    "print(bias1)\n",
    "# print(imagen1_obtenida)\n",
    "# print(veamos2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
