{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPROBACION DE FIRE MEDIANTE PYTORCH Y MEDIANTE NUMPY CON MATRIX MULTIPLICATIONS\n",
    "Partial code are copied heavily from https://github.com/pytorch/vision/blob/master/torchvision/models/squeezenet.py  \n",
    "SqueezeNet Paper:https://arxiv.org/abs/1602.07360  \n",
    "SqueezeNet 1.1 model from https://github.com/DeepScale/SqueezeNet/tree/master/SqueezeNet_v1.1   \n",
    "SqueezeNet 1.1 has 2.4x less computation and slightly fewer parameters than SqueezeNet 1.0, without sacrificing accuracy.\n",
    "\n",
    "Vamos a trabajar ya con fire1 del bloque 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#some set up\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import math\n",
    "import time\n",
    "from time import sleep, perf_counter as pc\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aquí tenemos el medidor entre pytorch y numpy adecuado para cada fire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- En esta aproximacion tengo en cuenta el tiempo de definición de capas y la carga de pesos porque se supone que entre fire y fire estas operaciones tiene que hacerse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 783.  829.  651.  733. 1077.]\n",
      "   [1421. 1053.  781.  773. 1211.]\n",
      "   [1163. 1425.  881.  857.  663.]\n",
      "   [ 495. 1359. 1593.  763.  815.]\n",
      "   [ 941.  787. 1035.  731. 1383.]]\n",
      "\n",
      "  [[ 502.  450.  403.  339.  580.]\n",
      "   [ 788.  511.  378.  443.  572.]\n",
      "   [ 599.  772.  374.  470.  256.]\n",
      "   [ 301.  739.  958.  417.  440.]\n",
      "   [ 530.  519.  451.  347.  808.]]\n",
      "\n",
      "  [[ 896.  946.  745.  835. 1228.]\n",
      "   [1620. 1199.  890.  883. 1378.]\n",
      "   [1325. 1624. 1002.  978.  754.]\n",
      "   [ 567. 1549. 1818.  871.  930.]\n",
      "   [1074.  901. 1177.  833. 1578.]]\n",
      "\n",
      "  [[ 270.  271.  222.  229.  351.]\n",
      "   [ 467.  333.  247.  257.  380.]\n",
      "   [ 374.  465.  269.  281.  198.]\n",
      "   [ 168.  444.  537.  250.  266.]\n",
      "   [ 311.  274.  318.  230.  462.]]\n",
      "\n",
      "  [[2628. 3551. 3120. 3352. 2787.]\n",
      "   [4479. 5596. 4701. 4464. 3688.]\n",
      "   [4491. 6204. 5626. 4938. 3237.]\n",
      "   [3566. 5882. 5859. 5191. 3232.]\n",
      "   [2374. 4007. 4107. 3690. 2929.]]\n",
      "\n",
      "  [[1515. 2792. 2156. 2142. 2057.]\n",
      "   [3016. 4853. 4492. 4042. 3418.]\n",
      "   [3351. 5199. 5366. 4811. 3294.]\n",
      "   [3338. 5351. 5315. 4646. 3122.]\n",
      "   [2099. 3741. 4060. 3612. 2023.]]\n",
      "\n",
      "  [[1612. 2837. 2419. 2569. 1999.]\n",
      "   [3121. 4449. 4192. 3571. 3007.]\n",
      "   [3396. 5026. 4864. 4320. 2684.]\n",
      "   [2782. 4978. 4783. 4466. 2545.]\n",
      "   [2014. 3174. 3541. 3546. 2277.]]\n",
      "\n",
      "  [[2250. 2924. 2713. 2945. 2250.]\n",
      "   [4133. 5870. 5126. 5205. 3586.]\n",
      "   [4583. 6757. 6373. 5387. 3436.]\n",
      "   [4355. 6664. 6616. 5538. 3386.]\n",
      "   [2959. 5325. 5388. 5764. 2967.]]]]\n",
      "[[[ 783.  829.  651.  733. 1077.]\n",
      "  [1421. 1053.  781.  773. 1211.]\n",
      "  [1163. 1425.  881.  857.  663.]\n",
      "  [ 495. 1359. 1593.  763.  815.]\n",
      "  [ 941.  787. 1035.  731. 1383.]]\n",
      "\n",
      " [[ 502.  450.  403.  339.  580.]\n",
      "  [ 788.  511.  378.  443.  572.]\n",
      "  [ 599.  772.  374.  470.  256.]\n",
      "  [ 301.  739.  958.  417.  440.]\n",
      "  [ 530.  519.  451.  347.  808.]]\n",
      "\n",
      " [[ 896.  946.  745.  835. 1228.]\n",
      "  [1620. 1199.  890.  883. 1378.]\n",
      "  [1325. 1624. 1002.  978.  754.]\n",
      "  [ 567. 1549. 1818.  871.  930.]\n",
      "  [1074.  901. 1177.  833. 1578.]]\n",
      "\n",
      " [[ 270.  271.  222.  229.  351.]\n",
      "  [ 467.  333.  247.  257.  380.]\n",
      "  [ 374.  465.  269.  281.  198.]\n",
      "  [ 168.  444.  537.  250.  266.]\n",
      "  [ 311.  274.  318.  230.  462.]]\n",
      "\n",
      " [[2628. 3551. 3120. 3352. 2787.]\n",
      "  [4479. 5596. 4701. 4464. 3688.]\n",
      "  [4491. 6204. 5626. 4938. 3237.]\n",
      "  [3566. 5882. 5859. 5191. 3232.]\n",
      "  [2374. 4007. 4107. 3690. 2929.]]\n",
      "\n",
      " [[1515. 2792. 2156. 2142. 2057.]\n",
      "  [3016. 4853. 4492. 4042. 3418.]\n",
      "  [3351. 5199. 5366. 4811. 3294.]\n",
      "  [3338. 5351. 5315. 4646. 3122.]\n",
      "  [2099. 3741. 4060. 3612. 2023.]]\n",
      "\n",
      " [[1612. 2837. 2419. 2569. 1999.]\n",
      "  [3121. 4449. 4192. 3571. 3007.]\n",
      "  [3396. 5026. 4864. 4320. 2684.]\n",
      "  [2782. 4978. 4783. 4466. 2545.]\n",
      "  [2014. 3174. 3541. 3546. 2277.]]\n",
      "\n",
      " [[2250. 2924. 2713. 2945. 2250.]\n",
      "  [4133. 5870. 5126. 5205. 3586.]\n",
      "  [4583. 6757. 6373. 5387. 3436.]\n",
      "  [4355. 6664. 6616. 5538. 3386.]\n",
      "  [2959. 5325. 5388. 5764. 2967.]]]\n",
      "tiempo en segundos con pytorch=  0.0008257096400120645\n",
      "tiempo en segundos con numpy=  0.00015660705997106562\n",
      "pytorch==numpy, total: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:51] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "canales_iniciales=4\n",
    "canales_contraidos=2\n",
    "canales_finales=4\n",
    "acumulado_pytorch=0\n",
    "acumulado_numpy=0\n",
    "idea=True\n",
    "count=100\n",
    "tamanyo=5\n",
    "squeeze_activation = nn.ReLU(inplace=True)\n",
    "\n",
    "for i in range(count):\n",
    "\n",
    "    imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "    weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "    bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "    weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "    bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "    weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "    bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "    tic=pc()\n",
    "    squeeze1=nn.Conv2d(canales_iniciales, canales_contraidos, kernel_size=1, bias=False)\n",
    "    squeeze1.weight = nn.Parameter(torch.from_numpy(weights1))\n",
    "    squeeze1.bias = nn.Parameter(torch.from_numpy(bias1))    \n",
    "\n",
    "    squeeze2a=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=1, bias=False)\n",
    "    squeeze2a.weight = nn.Parameter(torch.from_numpy(weights2a))\n",
    "    squeeze2a.bias = nn.Parameter(torch.from_numpy(bias2a))\n",
    "    \n",
    "    squeeze2b=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=3, bias=False, padding=1)\n",
    "    squeeze2b.weight = nn.Parameter(torch.from_numpy(weights2b))\n",
    "    squeeze2b.bias = nn.Parameter(torch.from_numpy(bias2b))    \n",
    "\n",
    "    imagen1  = torch.from_numpy(imagen).float()\n",
    "    \n",
    "    salida1=squeeze1(imagen1)\n",
    "    salida1_activation=squeeze_activation(salida1)\n",
    "    \n",
    "    salida2a=squeeze2a(salida1_activation)\n",
    "    salida2a_activation=squeeze_activation(salida2a)\n",
    "    salida2b=squeeze2b(salida1_activation)\n",
    "    salida2b_activation=squeeze_activation(salida2b)    \n",
    "    salida2_total=torch.cat([salida2a_activation,salida2b_activation], 1)\n",
    "    salida2_total_a_numpy=salida2_total.detach().numpy()\n",
    "    toc=pc()\n",
    "    acumulado_pytorch=toc-tic+acumulado_pytorch\n",
    "    \n",
    "    \n",
    "    tic2=pc()\n",
    "    n_h=1\n",
    "    n_w=1\n",
    "    pesos1=np.transpose(weights1, axes=(1, 0, 2, 3)).reshape(canales_iniciales*n_h*n_w, -1)\n",
    "    pesos2a=np.transpose(weights2a, axes=(1, 0, 2, 3)).reshape(canales_contraidos*n_h*n_w, -1)\n",
    "\n",
    "    n_h=3\n",
    "    n_w=3\n",
    "    idea2b_pre=weights2b.reshape(-1,canales_contraidos*n_h*n_w)\n",
    "    pesos2b=np.transpose(idea2b_pre)   \n",
    "    bt, c_i = imagen.shape[0:2] \n",
    "    s=1\n",
    "    n_h=1\n",
    "    n_w=1\n",
    "    k=imagen.shape[2]-n_h+1\n",
    "    col = np.zeros((bt, c_i , k,k, n_h, n_w))\n",
    "\n",
    "    for i in range(k):\n",
    "        i_max = i + s*n_h\n",
    "        for j in range(k):\n",
    "            j_max = j + s*n_w\n",
    "            col[:, :, i, j, :, :] = imagen[:, :, i:i_max:s, j:j_max:s]\n",
    "    # col = np.transpose(col, axes=(1, 4, 5, 0, 2, 3)).reshape(c_i*n_h*n_w, -1)\n",
    "    col = col.reshape(c_i*n_h*n_w, -1)\n",
    "    imagen1_numpy=np.transpose(col)\n",
    "    salida1_obtenida= np.dot(imagen1_numpy,pesos1)+bias1\n",
    "    salida1_obtenida_activacion=np.maximum(0,salida1_obtenida)\n",
    "    imagen1_obtenida = np.transpose(salida1_obtenida_activacion, axes=(1,0)).reshape(canales_contraidos,tamanyo,tamanyo)\n",
    "    imagen1_obtenida_ampliada=np.pad(imagen1_obtenida,((0,0),(1,1),(1,1)))\n",
    "    salida2a_obtenida= np.dot(salida1_obtenida,pesos2a)+bias2a\n",
    "    salida2a_obtenida_activacion=np.maximum(0,salida2a_obtenida)\n",
    "    c_i = imagen1_obtenida_ampliada.shape[0] \n",
    "    n_h=3\n",
    "    n_w=3\n",
    "    k=imagen1_obtenida_ampliada.shape[1]-n_h+1\n",
    "    col = np.zeros(( c_i , k,k, n_h, n_w))\n",
    "\n",
    "    for i in range(k):\n",
    "        i_max = i + s*n_h\n",
    "        for j in range(k):\n",
    "            j_max = j + s*n_w\n",
    "            col[ :, i, j, :, :] = imagen1_obtenida_ampliada[ :, i:i_max:s, j:j_max:s]\n",
    "    col = np.transpose(col, axes=(0, 3, 4, 1, 2)).reshape(c_i*n_h*n_w, -1)\n",
    "    #col = col.reshape(c_i*n_h*n_w, -1)\n",
    "    salida_obtenida_2b=np.transpose(col)\n",
    "    salida2b_obtenida= np.dot(salida_obtenida_2b,pesos2b)+bias2b\n",
    "    salida2b_obtenida_activacion=np.maximum(0,salida2b_obtenida)\n",
    "    salida2_total_numpy=np.concatenate((salida2a_obtenida_activacion,salida2b_obtenida_activacion), axis=1)\n",
    "    imagen_total_obtenida = np.transpose(salida2_total_numpy, axes=(1,0)).reshape(canales_finales*2,tamanyo,tamanyo)\n",
    "    toc2=pc()\n",
    "    acumulado_numpy=toc2-tic2+acumulado_numpy\n",
    "    idea&=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "print(salida2_total_a_numpy)\n",
    "print(imagen_total_obtenida)\n",
    "print (\"tiempo en segundos con pytorch= \", acumulado_pytorch/count)\n",
    "print (\"tiempo en segundos con numpy= \", acumulado_numpy/count)\n",
    "print (\"pytorch==numpy, total:\",idea)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Veamos ahora fire con opencl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vamos a hacer la medición con opencl definido por equipo de squezeenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step1: OpenCL preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# OpenCL setup\n",
    "import pyopencl as cl\n",
    "import deviceinfo\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyopencl.Context at 0x492c5b8 on <pyopencl.Device '12th Gen Intel(R) Core(TM) i7-12650H' on 'Intel(R) OpenCL' at 0x493ae08>>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "platforms = cl.get_platforms()\n",
    "context = cl.Context(\n",
    "        dev_type=cl.device_type.ALL,\n",
    "        properties=[(cl.context_properties.PLATFORM, platforms[0])])\n",
    "queue = cl.CommandQueue(context)\n",
    "queue_exp = cl.CommandQueue(context)\n",
    "\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamanyo=55\n",
    "canales_iniciales=128\n",
    "canales_contraidos=16\n",
    "canales_finales=64\n",
    "imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "\n",
    "weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: import parameters from pytorch implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 6. 9. ... 1. 2. 3.]\n",
      "[8. 2. 3. 5. 8. 1. 8. 5. 5. 1. 5. 8. 9. 6. 4. 7.]\n"
     ]
    }
   ],
   "source": [
    "# params = model.state_dict() \n",
    "\n",
    "# #remove # to see the params index\n",
    "# for k,v in params.items():\n",
    "#     #print parameter name\n",
    "#     print(k,params[k].numpy().shape)\n",
    "\n",
    "# conv1_weight = params['features.0.weight'].numpy().reshape(-1)\n",
    "# conv1_bias = params['features.0.bias'].numpy()\n",
    "\n",
    "#fire - fire - maxpool block 1\n",
    "fire1_squeeze_weight = weights1.reshape(-1)\n",
    "print(fire1_squeeze_weight)\n",
    "fire1_squeeze_bias = bias1\n",
    "print(fire1_squeeze_bias)\n",
    "fire1_expand1x1_weight = weights2a.reshape(-1)\n",
    "fire1_expand1x1_bias = bias2a\n",
    "\n",
    "fire1_expand3x3_weight =weights2b.reshape(-1)\n",
    "fire1_expand3x3_bias = bias2b\n",
    "\n",
    "# fire2_squeeze_weight = params['features.4.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire2_squeeze_bias = params['features.4.squeeze.bias'].numpy()\n",
    "\n",
    "# fire2_expand1x1_weight = params['features.4.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire2_expand1x1_bias = params['features.4.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire2_expand3x3_weight = params['features.4.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire2_expand3x3_bias = params['features.4.expand3x3.bias'].numpy()\n",
    "\n",
    "# #fire - fire - maxpool block 2\n",
    "# fire3_squeeze_weight = params['features.6.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire3_squeeze_bias = params['features.6.squeeze.bias'].numpy()\n",
    "\n",
    "# fire3_expand1x1_weight = params['features.6.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire3_expand1x1_bias = params['features.6.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire3_expand3x3_weight = params['features.6.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire3_expand3x3_bias = params['features.6.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire4_squeeze_weight = params['features.7.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire4_squeeze_bias = params['features.7.squeeze.bias'].numpy()\n",
    "\n",
    "# fire4_expand1x1_weight = params['features.7.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire4_expand1x1_bias = params['features.7.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire4_expand3x3_weight = params['features.7.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire4_expand3x3_bias = params['features.7.expand3x3.bias'].numpy()\n",
    "\n",
    "# #fire - fire - fire - fire block 3\n",
    "# fire5_squeeze_weight = params['features.9.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire5_squeeze_bias = params['features.9.squeeze.bias'].numpy()\n",
    "\n",
    "# fire5_expand1x1_weight = params['features.9.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire5_expand1x1_bias = params['features.9.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire5_expand3x3_weight = params['features.9.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire5_expand3x3_bias = params['features.9.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire6_squeeze_weight = params['features.10.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire6_squeeze_bias = params['features.10.squeeze.bias'].numpy()\n",
    "\n",
    "# fire6_expand1x1_weight = params['features.10.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire6_expand1x1_bias = params['features.10.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire6_expand3x3_weight = params['features.10.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire6_expand3x3_bias = params['features.10.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire7_squeeze_weight = params['features.11.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire7_squeeze_bias = params['features.11.squeeze.bias'].numpy()\n",
    "\n",
    "# fire7_expand1x1_weight = params['features.11.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire7_expand1x1_bias = params['features.11.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire7_expand3x3_weight = params['features.11.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire7_expand3x3_bias = params['features.11.expand3x3.bias'].numpy()\n",
    "\n",
    "# fire8_squeeze_weight = params['features.12.squeeze.weight'].numpy().reshape(-1)\n",
    "# fire8_squeeze_bias = params['features.12.squeeze.bias'].numpy()\n",
    "\n",
    "# fire8_expand1x1_weight = params['features.12.expand1x1.weight'].numpy().reshape(-1)\n",
    "# fire8_expand1x1_bias = params['features.12.expand1x1.bias'].numpy()\n",
    "\n",
    "# fire8_expand3x3_weight = params['features.12.expand3x3.weight'].numpy().reshape(-1)\n",
    "# fire8_expand3x3_bias = params['features.12.expand3x3.bias'].numpy()\n",
    "\n",
    "# classifier_conv_weight = params['classifier.1.weight'].numpy().reshape(-1)\n",
    "# classifier_conv_bias = params['classifier.1.bias'].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat OpenCL memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 8 5 ... 5 4 0]\n",
      "(387200,)\n"
     ]
    }
   ],
   "source": [
    "h_sample = imagen.reshape(-1)\n",
    "print(h_sample)\n",
    "print(h_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4624/3983008257.py:5: RuntimeWarning: overflow encountered in cast\n",
      "  h_result_fire1_squeeze = np.empty(1 * canales_contraidos * tamanyo * tamanyo).astype(np.float32)\n"
     ]
    }
   ],
   "source": [
    "h_sample = imagen.reshape(-1).astype(np.float32)\n",
    "# h_result_conv = np.empty(1 * 64 * 111 * 111).astype(np.float32)\n",
    "# h_result_pool1 = np.empty(1 * 64 * 55 * 55).astype(np.float32)\n",
    "\n",
    "h_result_fire1_squeeze = np.empty(1 * canales_contraidos * tamanyo * tamanyo).astype(np.float32)\n",
    "h_result_fire1_expand = np.empty(1 * 2*canales_finales * tamanyo* tamanyo).astype(np.float32)\n",
    "\n",
    "# h_result_fire2_squeeze = np.empty(1 * 16 * 55 * 55).astype(np.float32)\n",
    "# h_result_fire2_expand = np.empty(1 * 128 * 55 * 55).astype(np.float32)\n",
    "# h_result_pool2 = np.empty(1 * 128 * 27 * 27).astype(np.float32)\n",
    "\n",
    "# h_result_fire3_squeeze = np.empty(1 * 32 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire3_expand = np.empty(1 * 256 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire4_squeeze = np.empty(1 * 32 * 27 * 27).astype(np.float32)\n",
    "# h_result_fire4_expand = np.empty(1 * 256 * 27 * 27).astype(np.float32)\n",
    "# h_result_pool3 = np.empty(1 * 256 * 13 * 13).astype(np.float32)\n",
    "\n",
    "# h_result_fire5_squeeze = np.empty(1 * 48 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire5_expand = np.empty(1 * 384 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire6_squeeze = np.empty(1 * 48 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire6_expand = np.empty(1 * 384 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire7_squeeze = np.empty(1 * 64 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire7_expand = np.empty(1 * 512 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire8_squeeze = np.empty(1 * 64 * 13 * 13).astype(np.float32)\n",
    "# h_result_fire8_expand = np.empty(1 * 512 * 13 * 13).astype(np.float32)\n",
    "\n",
    "# h_result_classifier_conv = np.empty(1 * 1000 * 13 * 13).astype(np.float32)\n",
    "# h_result_classifier = np.empty(1 * 1000).astype(np.float32)\n",
    "\n",
    "# device input buffer\n",
    "d_sample = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=h_sample)\n",
    "\n",
    "# device conv1 buffers \n",
    "# d_conv1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_weight)\n",
    "# d_conv1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_bias)\n",
    "\n",
    "# d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights1)\n",
    "# d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias1)\n",
    "# d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2a)\n",
    "# d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2a)\n",
    "# d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2b)\n",
    "# d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2b)\n",
    "\n",
    "#print(d_fire1_squeeze_weight)\n",
    "d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_weight)\n",
    "d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_bias)\n",
    "d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_weight)\n",
    "d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_bias)\n",
    "d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_weight)\n",
    "d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_bias)\n",
    "\n",
    "# d_fire2_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_squeeze_weight)\n",
    "# d_fire2_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_squeeze_bias)\n",
    "# d_fire2_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand1x1_weight)\n",
    "# d_fire2_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand1x1_bias)\n",
    "# d_fire2_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand3x3_weight)\n",
    "# d_fire2_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire2_expand3x3_bias)\n",
    "\n",
    "# d_fire3_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_squeeze_weight)\n",
    "# d_fire3_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_squeeze_bias)\n",
    "# d_fire3_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand1x1_weight)\n",
    "# d_fire3_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand1x1_bias)\n",
    "# d_fire3_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand3x3_weight)\n",
    "# d_fire3_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire3_expand3x3_bias)\n",
    "\n",
    "# d_fire4_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_squeeze_weight)\n",
    "# d_fire4_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_squeeze_bias)\n",
    "# d_fire4_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand1x1_weight)\n",
    "# d_fire4_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand1x1_bias)\n",
    "# d_fire4_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand3x3_weight)\n",
    "# d_fire4_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire4_expand3x3_bias)\n",
    "\n",
    "# d_fire5_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_squeeze_weight)\n",
    "# d_fire5_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_squeeze_bias)\n",
    "# d_fire5_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand1x1_weight)\n",
    "# d_fire5_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand1x1_bias)\n",
    "# d_fire5_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand3x3_weight)\n",
    "# d_fire5_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire5_expand3x3_bias)\n",
    "\n",
    "# d_fire6_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_squeeze_weight)\n",
    "# d_fire6_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_squeeze_bias)\n",
    "# d_fire6_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand1x1_weight)\n",
    "# d_fire6_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand1x1_bias)\n",
    "# d_fire6_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand3x3_weight)\n",
    "# d_fire6_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire6_expand3x3_bias)\n",
    "\n",
    "# d_fire7_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_squeeze_weight)\n",
    "# d_fire7_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_squeeze_bias)\n",
    "# d_fire7_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand1x1_weight)\n",
    "# d_fire7_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand1x1_bias)\n",
    "# d_fire7_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand3x3_weight)\n",
    "# d_fire7_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire7_expand3x3_bias)\n",
    "\n",
    "# d_fire8_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_squeeze_weight)\n",
    "# d_fire8_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_squeeze_bias)\n",
    "# d_fire8_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand1x1_weight)\n",
    "# d_fire8_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand1x1_bias)\n",
    "# d_fire8_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand3x3_weight)\n",
    "# d_fire8_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire8_expand3x3_bias)\n",
    "\n",
    "# d_classifier_conv_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=classifier_conv_weight)\n",
    "# d_classifier_conv_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=classifier_conv_bias)\n",
    "\n",
    "# d_result_conv = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_conv.nbytes)\n",
    "# d_result_pool1 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool1.nbytes)\n",
    "\n",
    "d_result_fire1_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire1_squeeze.nbytes)\n",
    "d_result_fire1_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire1_expand.nbytes)\n",
    "# d_result_fire2_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire2_squeeze.nbytes)\n",
    "# d_result_fire2_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire2_expand.nbytes)\n",
    "# d_result_pool2 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool2.nbytes)\n",
    "\n",
    "# d_result_fire3_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire3_squeeze.nbytes)\n",
    "# d_result_fire3_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire3_expand.nbytes)\n",
    "# d_result_fire4_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire4_squeeze.nbytes)\n",
    "# d_result_fire4_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire4_expand.nbytes)\n",
    "# d_result_pool3 = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_pool3.nbytes)\n",
    "\n",
    "# d_result_fire5_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire5_squeeze.nbytes)\n",
    "# d_result_fire5_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire5_expand.nbytes)\n",
    "# d_result_fire6_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire6_squeeze.nbytes)\n",
    "# d_result_fire6_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire6_expand.nbytes)\n",
    "# d_result_fire7_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire7_squeeze.nbytes)\n",
    "# d_result_fire7_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire7_expand.nbytes)\n",
    "# d_result_fire8_squeeze = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire8_squeeze.nbytes)\n",
    "# d_result_fire8_expand = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_fire8_expand.nbytes)\n",
    "\n",
    "# d_result_classifier_conv = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_classifier_conv.nbytes)\n",
    "# d_result_classifier = cl.Buffer(context, cl.mem_flags.WRITE_ONLY, h_result_classifier.nbytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: creat kernels\n",
    "Creat & build program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joerock/anaconda3/envs/TFM/lib/python3.8/site-packages/pyopencl/__init__.py:270: CompilerWarning: Non-empty compiler output encountered. Set the environment variable PYOPENCL_COMPILER_OUTPUT=1 to see more.\n",
      "  warn(\"Non-empty compiler output encountered. Set the \"\n"
     ]
    }
   ],
   "source": [
    "kernelSource = open(\"squeezenet.cl\").read()\n",
    "program = cl.Program(context, kernelSource).build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creat kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv3x3 = program.conv2d3x3\n",
    "conv3x3.set_scalar_arg_dtypes([np.int32, np.int32, np.int32, np.int32, np.int32, np.int32, \\\n",
    "                               None, None, None, None])\n",
    "\n",
    "maxpool = program.maxpool2d\n",
    "maxpool.set_scalar_arg_dtypes([np.int32, np.int32, None, None])\n",
    "\n",
    "conv1x1 = program.conv2d1x1_ST\n",
    "conv1x1.set_scalar_arg_dtypes([np.int32, np.int32, np.int32, \\\n",
    "                               None, None, None, None])\n",
    "\n",
    "avgpool = program.avgpool2d\n",
    "avgpool.set_scalar_arg_dtypes([None, None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenCL kernel: squeezenet.cl\n",
    "conv2d3x3: 2-D 3x3 convolution.  \n",
    "conv2d1x1: 2-D 1x1 convolution. kerner size 1, stride 1  \n",
    "maxpool2d: 2-D max pool. kerner size 3, stride 2  \n",
    "avgpool2d: 2-D average pool. kerner size 13\n",
    "```C\n",
    "//maxPool2d \n",
    "//kernel_size=3 stride=2\n",
    "//output one feature map per kernel\n",
    "__kernel void maxpool2d(\n",
    "\tconst int input_size,\n",
    "\tconst int output_size,\n",
    "\t__global float *input_im,\n",
    "    __global float *restrict output_im)\n",
    "{\n",
    "\tint channel = get_global_id(0);//get output channel index\n",
    "\t\n",
    "\tinput_im += channel * input_size * input_size;\n",
    "\toutput_im += channel * output_size * output_size;\n",
    "\n",
    "\t//loop over output feature map\n",
    "\tfor(int i = 0; i < output_size; i++)//row\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < output_size; j++)//col\n",
    "\t\t{\n",
    "\t\t\t//find the max value in 3x3 reigon \n",
    "\t\t\t//to be one element in the output feature map\n",
    "\t\t\tfloat tmp = 0.0;\n",
    "\n",
    "\t\t\tfor(int k = 0; k < 3; k++)//row\n",
    "\t\t\t{\n",
    "\t\t\t\tfor(int l = 0; l < 3; l++)//col\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tfloat value = input_im[(i * 2 + k) * input_size  + j * 2 + l ];\n",
    "\t\t\t\t\tif(value > tmp)\n",
    "\t\t\t\t\t\ttmp = value;\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\t\t\t//store the result to output feature map\n",
    "\t\t\toutput_im[i * output_size + j] = tmp; \n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//3x3 convolution layer\n",
    "//output one feature map per kernel\n",
    "__kernel void conv2d3x3(\n",
    "\tconst int input_channels, const int input_size,\n",
    "\tconst int pad, const int stride,\n",
    "\tconst int start_channel, //start_channel is for 1x1 feature map in fire layer\n",
    "\tconst int output_size,\n",
    "\t__global float *restrict input_im,\n",
    "\t__global const float *restrict  filter_weight,\n",
    "\t__global const float *restrict  filter_bias,\n",
    "\t__global float *restrict output_im\n",
    "\t)\n",
    "{\n",
    "\tint filter_index = get_global_id(0); //get output channel index\n",
    "\n",
    "\tfilter_weight += filter_index * input_channels * 9;\n",
    "\tfloat bias = filter_bias[filter_index];\n",
    "\toutput_im += (start_channel + filter_index) * output_size * output_size;\n",
    "\t\n",
    "\t//loop over output feature map\n",
    "\tfor(int i = 0; i < output_size; i++)\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < output_size; j++)\n",
    "\t\t{\n",
    "\t\t\t//compute one element in the output feature map\n",
    "\t\t\tfloat tmp = bias;\n",
    "\t\t\t\n",
    "\t\t\t//compute dot product of 2 input_channels x 3 x 3 matrix\n",
    "\t\t\tfor(int k = 0; k < input_channels; k++)\n",
    "\t\t\t{\n",
    "\t\t\t\tfor(int l = 0; l < 3; l++)\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\tint h = i * stride + l - pad;\n",
    "\t\t\t\t\tfor(int m = 0; m < 3; m++)\n",
    "\t\t\t\t\t{\n",
    "\t\t\t\t\t\tint w = j * stride + m - pad;\n",
    "\t\t\t\t\t\tif((h >= 0) && (h < input_size) && (w >= 0) && (w < input_size))\n",
    "\t\t\t\t\t\t{\n",
    "\t\t\t\t\t\t\ttmp += input_im[k * input_size * input_size + (i * stride + l - pad) * input_size + j \n",
    "                                   * stride + m - pad] * filter_weight[9 * k + 3 * l + m];\n",
    "\t\t\t\t\t\t}\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t}\n",
    "\n",
    "\t\t\t//add relu activation after conv\n",
    "\t\t\toutput_im[i * output_size + j] = (tmp > 0.0) ? tmp : 0.0;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//1x1 convolution layer\n",
    "//output one feature map per kernel\n",
    "__kernel void conv2d1x1(\n",
    "\tconst int input_channels, const int input_size,\n",
    "\t__global float *input_im,\n",
    "\t__global const float *restrict filter_weight,\n",
    "\t__global const float *restrict filter_bias,\n",
    "\t__global float *restrict output_im)\n",
    "{\n",
    "\tint filter_index = get_global_id(0); // 0 - (output_channels - 1)\n",
    "\n",
    "\tfilter_weight += filter_index * input_channels;\n",
    "\n",
    "\tfloat bias = filter_bias[filter_index];\n",
    "\t\n",
    "\toutput_im += filter_index * input_size * input_size;//start_channel is for 1x1 feature map in fire layer\n",
    "\n",
    "\t//loop over output feature map\n",
    "\t//out\n",
    "\tfor(int i = 0; i < input_size; i++)\n",
    "\t{\n",
    "\t\tfor(int j = 0; j < input_size; j++)\n",
    "\t\t{\n",
    "\t\t\tfloat tmp = bias;\n",
    "\t\t\tfor(int k = 0; k < input_channels; k++)\n",
    "\t\t\t{\n",
    "\t\t\t\ttmp += input_im[k * input_size * input_size + i * input_size + j] * filter_weight[k];\n",
    "\t\t\t}\n",
    "\t\t\t//add relu after conv\n",
    "\t\t\toutput_im[i * input_size + j] = (tmp > 0.0) ? tmp : 0.0;\n",
    "\t\t}\n",
    "\t}\n",
    "}\n",
    "\n",
    "//last layer use a 13 x 13 avgPool layer as classifier\n",
    "//one class score per kernel\n",
    "__kernel void avgpool2d(\n",
    "\t__global float *restrict input_im,\n",
    "\t__global float *restrict output_im)\n",
    "{\n",
    "\tint class_index = get_global_id(0);//get class score index\n",
    "\n",
    "\tinput_im += 169 * class_index;\n",
    "\t\n",
    "\tfloat tmp = 0.0f;\n",
    "\n",
    "\tfor(int i = 0; i < 169; i++)\n",
    "\t{\n",
    "\t\ttmp += input_im[i];\n",
    "\t}\n",
    "\n",
    "\toutput_im[class_index] = tmp / 169.0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run OpenCL implement  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiempo en segundos con pytorch=  0.004314813999826583\n",
      "tiempo en segundos con numpy=  0.048387223000190716\n",
      "tiempo en segundos con opencl= 0.04901780800037159\n",
      "comparativa (pytorch==numpy): True\n",
      "comparativa (pytorch==opencl): True\n"
     ]
    }
   ],
   "source": [
    "imagen = np.random.randint(10,size=(1,canales_iniciales, tamanyo, tamanyo))\n",
    "\n",
    "weights1=np.random.randint(10,size=(canales_contraidos, canales_iniciales,1,1)).astype(np.float32)\n",
    "bias1=np.random.randint(10,size=(canales_contraidos,)).astype(np.float32)\n",
    "weights2a=np.random.randint(10,size=(canales_finales, canales_contraidos,1,1)).astype(np.float32)  \n",
    "bias2a=np.random.randint(10,size=(canales_finales,)).astype(np.float32)    \n",
    "weights2b=np.random.randint(10,size=(canales_finales, canales_contraidos,3,3)).astype(np.float32)    \n",
    "bias2b=np.random.randint(10,size=(canales_finales,)).astype(np.float32)  \n",
    "\n",
    "tic=pc()\n",
    "squeeze1=nn.Conv2d(canales_iniciales, canales_contraidos, kernel_size=1, bias=False)\n",
    "squeeze1.weight = nn.Parameter(torch.from_numpy(weights1))\n",
    "squeeze1.bias = nn.Parameter(torch.from_numpy(bias1))\n",
    "\n",
    "squeeze2a=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=1, bias=False)\n",
    "squeeze2a.weight = nn.Parameter(torch.from_numpy(weights2a))\n",
    "squeeze2a.bias = nn.Parameter(torch.from_numpy(bias2a))\n",
    "\n",
    "squeeze2b=nn.Conv2d(canales_contraidos, canales_finales, kernel_size=3, bias=False, padding=1)\n",
    "squeeze2b.weight = nn.Parameter(torch.from_numpy(weights2b))\n",
    "squeeze2b.bias = nn.Parameter(torch.from_numpy(bias2b))\n",
    "\n",
    "\n",
    "imagen1  = torch.from_numpy(imagen).float()\n",
    "\n",
    "salida1=squeeze1(imagen1)\n",
    "salida1_activation=squeeze_activation(salida1)\n",
    "\n",
    "salida2a=squeeze2a(salida1_activation)\n",
    "salida2a_activation=squeeze_activation(salida2a)\n",
    "salida2b=squeeze2b(salida1_activation)\n",
    "salida2b_activation=squeeze_activation(salida2b)    \n",
    "salida2_total=torch.cat([salida2a_activation,salida2b_activation], 1)\n",
    "salida2_total_a_numpy=salida2_total.detach().numpy()\n",
    "toc=pc()\n",
    "acumulado_pytorch=toc-tic\n",
    "\n",
    "\n",
    "tic2=pc()\n",
    "n_h=1\n",
    "n_w=1\n",
    "pesos1=np.transpose(weights1, axes=(1, 0, 2, 3)).reshape(canales_iniciales*n_h*n_w, -1)\n",
    "pesos2a=np.transpose(weights2a, axes=(1, 0, 2, 3)).reshape(canales_contraidos*n_h*n_w, -1)\n",
    "\n",
    "n_h=3\n",
    "n_w=3\n",
    "idea2b_pre=weights2b.reshape(-1,canales_contraidos*n_h*n_w)\n",
    "pesos2b=np.transpose(idea2b_pre)   \n",
    "bt, c_i = imagen.shape[0:2] \n",
    "s=1\n",
    "n_h=1\n",
    "n_w=1\n",
    "k=imagen.shape[2]-n_h+1\n",
    "col = np.zeros((bt, c_i , k,k, n_h, n_w))\n",
    "\n",
    "for i in range(k):\n",
    "    i_max = i + s*n_h\n",
    "    for j in range(k):\n",
    "        j_max = j + s*n_w\n",
    "        col[:, :, i, j, :, :] = imagen[:, :, i:i_max:s, j:j_max:s]\n",
    "# col = np.transpose(col, axes=(1, 4, 5, 0, 2, 3)).reshape(c_i*n_h*n_w, -1)\n",
    "col = col.reshape(c_i*n_h*n_w, -1)\n",
    "imagen1_numpy=np.transpose(col)\n",
    "salida1_obtenida= np.dot(imagen1_numpy,pesos1)+bias1\n",
    "salida1_obtenida_activacion=np.maximum(0,salida1_obtenida)\n",
    "imagen1_obtenida = np.transpose(salida1_obtenida_activacion, axes=(1,0)).reshape(canales_contraidos,tamanyo,tamanyo)\n",
    "imagen1_obtenida_ampliada=np.pad(imagen1_obtenida,((0,0),(1,1),(1,1)))\n",
    "salida2a_obtenida= np.dot(salida1_obtenida,pesos2a)+bias2a\n",
    "salida2a_obtenida_activacion=np.maximum(0,salida2a_obtenida)\n",
    "c_i = imagen1_obtenida_ampliada.shape[0] \n",
    "n_h=3\n",
    "n_w=3\n",
    "k=imagen1_obtenida_ampliada.shape[1]-n_h+1\n",
    "col = np.zeros(( c_i , k,k, n_h, n_w))\n",
    "\n",
    "for i in range(k):\n",
    "    i_max = i + s*n_h\n",
    "    for j in range(k):\n",
    "        j_max = j + s*n_w\n",
    "        col[ :, i, j, :, :] = imagen1_obtenida_ampliada[ :, i:i_max:s, j:j_max:s]\n",
    "col = np.transpose(col, axes=(0, 3, 4, 1, 2)).reshape(c_i*n_h*n_w, -1)\n",
    "#col = col.reshape(c_i*n_h*n_w, -1)\n",
    "salida_obtenida_2b=np.transpose(col)\n",
    "salida2b_obtenida= np.dot(salida_obtenida_2b,pesos2b)+bias2b\n",
    "salida2b_obtenida_activacion=np.maximum(0,salida2b_obtenida)\n",
    "salida2_total_numpy=np.concatenate((salida2a_obtenida_activacion,salida2b_obtenida_activacion), axis=1)\n",
    "imagen_total_obtenida = np.transpose(salida2_total_numpy, axes=(1,0)).reshape(canales_finales*2,tamanyo,tamanyo)\n",
    "toc2=pc()\n",
    "acumulado_numpy=toc2-tic2\n",
    "idea&=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "\n",
    "\n",
    "# device conv1 buffers \n",
    "# d_conv1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_weight)\n",
    "# d_conv1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=conv1_bias)\n",
    "\n",
    "# d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights1)\n",
    "# d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias1)\n",
    "# d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2a)\n",
    "# d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2a)\n",
    "# d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=weights2b)\n",
    "# d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=bias2b)\n",
    "\n",
    "h_sample = imagen.reshape(-1).astype(np.float32)\n",
    "d_sample = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=h_sample)\n",
    "\n",
    "fire1_squeeze_weight = weights1.reshape(-1)\n",
    "fire1_squeeze_bias = bias1\n",
    "fire1_expand1x1_weight = weights2a.reshape(-1)\n",
    "fire1_expand1x1_bias = bias2a\n",
    "fire1_expand3x3_weight =weights2b.reshape(-1)\n",
    "fire1_expand3x3_bias = bias2b\n",
    "\n",
    "d_fire1_squeeze_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_weight)\n",
    "d_fire1_squeeze_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_squeeze_bias)\n",
    "d_fire1_expand1x1_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_weight)\n",
    "d_fire1_expand1x1_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand1x1_bias)\n",
    "d_fire1_expand3x3_weight = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_weight)\n",
    "d_fire1_expand3x3_bias = cl.Buffer(context, cl.mem_flags.READ_ONLY | cl.mem_flags.COPY_HOST_PTR, hostbuf=fire1_expand3x3_bias)\n",
    "\n",
    "tic3 = pc()\n",
    "#first conv layer\n",
    "# conv3x3(queue,(64,), None, 3, 224, 0, 2, 0, 111, d_sample, d_conv1_weight, d_conv1_bias, d_result_conv)\n",
    "# maxpool(queue,(64,), None, 111, 55, d_result_conv, d_result_pool1)\n",
    "\n",
    "#block1\n",
    "conv1x1(queue,(1,), None, np.int32(canales_iniciales/4), tamanyo, canales_contraidos, d_sample, d_fire1_squeeze_weight, d_fire1_squeeze_bias, d_result_fire1_squeeze)\n",
    "queue.finish()\n",
    "conv1x1(queue,(1,), None, np.int32(canales_contraidos/4), tamanyo, canales_finales, d_result_fire1_squeeze, d_fire1_expand1x1_weight, d_fire1_expand1x1_bias, d_result_fire1_expand)\n",
    "conv3x3(queue_exp,(canales_finales, tamanyo), None, canales_contraidos, tamanyo, 1, 1, canales_finales, tamanyo, d_result_fire1_squeeze, d_fire1_expand3x3_weight, d_fire1_expand3x3_bias, d_result_fire1_expand)\n",
    "queue.finish()\n",
    "queue_exp.finish()\n",
    "# conv1x1(queue,(16,), None, 128, 55, d_result_fire1_expand, d_fire2_squeeze_weight, d_fire2_squeeze_bias, d_result_fire2_squeeze)\n",
    "# conv1x1(queue,(64,), None, 16, 55, d_result_fire2_squeeze, d_fire2_expand1x1_weight, d_fire2_expand1x1_bias, d_result_fire2_expand)\n",
    "# conv3x3(queue,(64,), None, 16, 55, 1, 1, 64, 55, d_result_fire2_squeeze, d_fire2_expand3x3_weight, d_fire2_expand3x3_bias, d_result_fire2_expand)\n",
    "\n",
    "# maxpool(queue,(128,), None, 55, 27, d_result_fire2_expand, d_result_pool2)\n",
    "\n",
    "# #block2\n",
    "# conv1x1(queue,(32,), None, 128, 27, d_result_pool2, d_fire3_squeeze_weight, d_fire3_squeeze_bias, d_result_fire3_squeeze)\n",
    "# conv1x1(queue,(128,), None, 32, 27, d_result_fire3_squeeze, d_fire3_expand1x1_weight, d_fire3_expand1x1_bias, d_result_fire3_expand)\n",
    "# conv3x3(queue,(128,), None, 32, 27, 1, 1, 128, 27, d_result_fire3_squeeze, d_fire3_expand3x3_weight, d_fire3_expand3x3_bias, d_result_fire3_expand)\n",
    "\n",
    "# conv1x1(queue,(32,), None, 256, 27, d_result_fire3_expand, d_fire4_squeeze_weight, d_fire4_squeeze_bias, d_result_fire4_squeeze)\n",
    "# conv1x1(queue,(128,), None, 32, 27, d_result_fire4_squeeze, d_fire4_expand1x1_weight, d_fire4_expand1x1_bias, d_result_fire4_expand)\n",
    "# conv3x3(queue,(128,), None, 32, 27, 1, 1, 128, 27, d_result_fire4_squeeze, d_fire4_expand3x3_weight, d_fire4_expand3x3_bias, d_result_fire4_expand)\n",
    "\n",
    "# maxpool(queue,(256,), None, 27, 13, d_result_fire4_expand, d_result_pool3)\n",
    "\n",
    "# #block3\n",
    "# conv1x1(queue,(48,), None, 256, 13, d_result_pool3, d_fire5_squeeze_weight, d_fire5_squeeze_bias, d_result_fire5_squeeze)\n",
    "# conv1x1(queue,(192,), None, 48, 13, d_result_fire5_squeeze, d_fire5_expand1x1_weight, d_fire5_expand1x1_bias, d_result_fire5_expand)\n",
    "# conv3x3(queue,(192,), None, 48, 13, 1, 1, 192, 13, d_result_fire5_squeeze, d_fire5_expand3x3_weight, d_fire5_expand3x3_bias, d_result_fire5_expand)\n",
    "\n",
    "# conv1x1(queue,(48,), None, 384, 13, d_result_fire5_expand, d_fire6_squeeze_weight, d_fire6_squeeze_bias, d_result_fire6_squeeze)\n",
    "# conv1x1(queue,(192,), None, 48, 13, d_result_fire6_squeeze, d_fire6_expand1x1_weight, d_fire6_expand1x1_bias, d_result_fire6_expand)\n",
    "# conv3x3(queue,(192,), None, 48, 13, 1, 1, 192, 13, d_result_fire6_squeeze, d_fire6_expand3x3_weight, d_fire6_expand3x3_bias, d_result_fire6_expand)\n",
    "\n",
    "# conv1x1(queue,(64,), None, 384, 13, d_result_fire6_expand, d_fire7_squeeze_weight, d_fire7_squeeze_bias, d_result_fire7_squeeze)\n",
    "# conv1x1(queue,(256,), None, 64, 13, d_result_fire7_squeeze, d_fire7_expand1x1_weight, d_fire7_expand1x1_bias, d_result_fire7_expand)\n",
    "# conv3x3(queue,(256,), None, 64, 13, 1, 1, 256, 13, d_result_fire7_squeeze, d_fire7_expand3x3_weight, d_fire7_expand3x3_bias, d_result_fire7_expand)\n",
    "\n",
    "# conv1x1(queue,(64,), None, 512, 13, d_result_fire7_expand, d_fire8_squeeze_weight, d_fire8_squeeze_bias, d_result_fire8_squeeze)\n",
    "# conv1x1(queue,(256,), None, 64, 13, d_result_fire8_squeeze, d_fire8_expand1x1_weight, d_fire8_expand1x1_bias, d_result_fire8_expand)\n",
    "# conv3x3(queue,(256,), None, 64, 13, 1, 1, 256, 13, d_result_fire8_squeeze, d_fire8_expand3x3_weight, d_fire8_expand3x3_bias, d_result_fire8_expand)\n",
    "\n",
    "# # classifier\n",
    "# conv1x1(queue,(1000,), None, 512, 13, d_result_fire8_expand, d_classifier_conv_weight, d_classifier_conv_bias, d_result_classifier_conv)\n",
    "# avgpool(queue,(1000,), None, d_result_classifier_conv, d_result_classifier)\n",
    "# # Wait for the commands to finish before reading back\n",
    "# event3.wait()\n",
    "#queue.finish()\n",
    "#cl.enqueue_copy(queue, h_result_fire1_squeeze,  d_result_fire1_squeeze)\n",
    "cl.enqueue_copy(queue, h_result_fire1_expand, d_result_fire1_expand)\n",
    "queue.finish()\n",
    "veamos=h_result_fire1_expand.reshape(-1,tamanyo,tamanyo)\n",
    "#veamos2=h_result_fire1_squeeze.reshape(-1,tamanyo,tamanyo)\n",
    "rtime = pc() - tic3\n",
    "print (\"tiempo en segundos con pytorch= \", toc-tic)\n",
    "print (\"tiempo en segundos con numpy= \", toc2-tic2)\n",
    "print (\"tiempo en segundos con opencl=\",rtime)\n",
    "\n",
    "comparativa1=np.allclose(salida2_total_a_numpy, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "comparativa2=np.allclose(veamos, imagen_total_obtenida,rtol=1e-01, atol=1e-01)\n",
    "print(\"comparativa (pytorch==numpy):\",comparativa1)\n",
    "print(\"comparativa (pytorch==opencl):\",comparativa2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 55, 55)\n",
      "(128, 55, 55)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(imagen_total_obtenida.shape)\n",
    "print(veamos.shape)\n",
    "# print(imagen)\n",
    "# print(weights1)\n",
    "# print(fire1_squeeze_weight)\n",
    "# print(bias1)\n",
    "# print(imagen1_obtenida)\n",
    "# print(veamos2)\n",
    "tamanyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(128):\n",
    "    for j in range(55):\n",
    "        for k in range(55):\n",
    "            if (abs(imagen_total_obtenida[i][j][k] - veamos[i][j][k])) > 1e-01:\n",
    "                print(\"i:\", i, \"j:\", j, \"k:\", k, imagen_total_obtenida[i][j][k], veamos[i][j][k])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
